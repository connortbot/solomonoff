"""
Filename: attention.py

Description: Implements attention sublayer (Multi-Head Attention) and helpers such as GQA scaled dot product attentino.

Notes:

"""

import math
import torch.nn as nn


class MultiHeadAttention(nn.Module):
    def __init__(self):
        pass

if __name__ == "__main__":
    pass